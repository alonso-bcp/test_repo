{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3f5a060-1bc4-4578-b660-e48ce4a71635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "sys.path.append('/Workspace/Users/eliaszachary@bcp.com.pe/Modulo de Seguimiento/')\n",
    "\n",
    "# Librerias y fuciones generales\n",
    "from pyspark.sql.functions import date_format, expr, to_date, date_sub, add_months, col, when, coalesce, trim, broadcast, avg, max, min, lit, concat, window, round as colround, upper, abs as sparkabs,greatest\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import StorageLevel\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Mostrar los duplicados\n",
    "def print_res(sparkf_df):\n",
    "  import pandas as pd\n",
    "  pd.set_option('display.max_rows', None)\n",
    "  pd.set_option('display.max_columns', None)\n",
    "  pd_spark_df = sparkf_df.toPandas()\n",
    " \n",
    "  return pd_spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c93cb45-610e-4ab3-9a78-60bcfd16ee84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Base de seguimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "708222cd-5b15-4262-87fd-564a4b5e6f67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, expr, to_date, date_sub, add_months, col, when, coalesce, trim\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Omitir todas las advertencias\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Parámetros de guardado\n",
    "vUsuario = 'T70431'\n",
    "vP = 'Em1z4marrod'\n",
    "vNumReg = 10000000\n",
    "vStorage = \"dtbrcu1lhclp02\"\n",
    "vRutaEdvDestino = \"bcp-edv-fabseg/T70431/Bases DWH/Base Applicant Hipotecario\"\n",
    "vContainer = vRutaEdvDestino.split(\"/\", 1)[0]\n",
    "vRutaEDV = vRutaEdvDestino[len(vContainer) : len(vRutaEdvDestino)]\n",
    "vQueryOrigen = \"SELECT * FROM S35105.HM_MTZ_ADM_HIPOTECARIO_DRIVER WHERE CODMES >= 201601\"\n",
    "\n",
    "try:\n",
    "    # Lectura de la base de hipotecario\n",
    "    base_app_veh = (\n",
    "        spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", \"jdbc:oracle:thin:@PRODREG.credito.bcp.com.pe:1521/BCPDW3\")       \n",
    "        .option(\"dbtable\", \"(SELECT ROWNUM as R, t.* FROM (\"+vQueryOrigen+\") t ) mitabla\")\n",
    "        .option(\"fetchSize\",100000) \n",
    "        .option(\"partitionColumn\", \"R\")\n",
    "        .option(\"lowerBound\", \"1\")\n",
    "        .option(\"upperBound\", vNumReg)\n",
    "        .option(\"numPartitions\", \"50\")\n",
    "        .option(\"user\", vUsuario)\n",
    "        .option(\"password\", vP)\n",
    "        .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\")\n",
    "        .option(\"oracle.jdbc.timezoneAsRegion\", \"false\")\n",
    "        .option(\"oracle.jdbc.defaultCharSet\", \"UTF-8\")\n",
    "        .load()\n",
    "    )\n",
    "    print(f\"Conexión exitosa: consulta '{vQueryOrigen}' leida correctamente.\")\n",
    "\n",
    "    # Guardar el DataFrame en un contenedor de Azure Storage Account\n",
    "    base_app_veh.write.format(\"delta\").mode(\"overwrite\").save(\"abfss://\"+vContainer+\"@\"+vStorage+\".dfs.core.windows.net\"+vRutaEDV)\n",
    "\n",
    "    spark.sql(\n",
    "    '''\n",
    "        DROP TABLE IF EXISTS catalog_lhcl_prod_bcp.bcp_edv_fabseg.T70431_BASE_APP_HIP\n",
    "    '''\n",
    "    )\n",
    "    spark.sql('''\n",
    "        CREATE TABLE \n",
    "            catalog_lhcl_prod_bcp.bcp_edv_fabseg.T70431_BASE_APP_HIP\n",
    "        USING DELTA LOCATION \n",
    "            'abfss://bcp-edv-fabseg@adlscu1lhclbackp05.dfs.core.windows.net/T70431/Bases DWH/Base Applicant Hipotecario' \n",
    "    '''\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Conexión fallida: {e}\")\n",
    "\n",
    "\n",
    "        #    'abfss://bcp-edv-fabseg@adlscu1lhclbackp05.dfs.core.windows.net/T70431/Bases DWH/Base Applicant Hipotecario' colocar nueva ruta de azure\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-08-27 09:34:13",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
